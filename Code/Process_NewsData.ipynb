{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 574,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from string import punctuation\n",
    "import random\n",
    "import math\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import movie_reviews\n",
    "from nltk import word_tokenize\n",
    "from nltk.util import ngrams\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem import SnowballStemmer\n",
    "from spellchecker import SpellChecker\n",
    "from datetime import datetime, timedelta\n",
    "import re,string,math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 575,
   "metadata": {},
   "outputs": [],
   "source": [
    "def change_dual_words(word):\n",
    "    first=''\n",
    "    second=''\n",
    "    def get_correct_word(w):\n",
    "        if w==\"sha\":\n",
    "            return(\"shall\")\n",
    "        elif w==\"wo\":\n",
    "            return(\"will\")\n",
    "        elif w==\"y\":\n",
    "            return(\"you\")\n",
    "        else:\n",
    "            return(w)\n",
    "#     word=word.replace('$','$').replace('+','')\n",
    "    word=word.strip().strip(':').strip(\"'\").strip(',').strip('`').strip('[').strip(']').strip('-').strip('_').strip('#').lower()\n",
    "    word=word.strip('?').strip('+').strip('/').strip('(').strip(')').strip(';').strip('!').strip('.').strip('?')\n",
    "    if word.endswith(\"n't\"):\n",
    "        first=get_correct_word(word[:word.index(\"n't\")])\n",
    "        second=\"not\"\n",
    "    elif word.endswith(\"'ll\"):\n",
    "        first=get_correct_word(word[:word.index(\"'ll\")])\n",
    "        second=\"will\"\n",
    "    elif word.endswith(\"'ve\"):\n",
    "        first=get_correct_word(word[:word.index(\"'ve\")])\n",
    "        second=\"have\"\n",
    "    elif word.endswith(\"'re\"):\n",
    "        first=get_correct_word(word[:word.index(\"'re\")])\n",
    "        second=\"are\"\n",
    "    elif word.endswith(\"'d\"):\n",
    "        first=get_correct_word(word[:word.index(\"'d\")])\n",
    "        second=\"did\"\n",
    "    elif word.endswith(\"'s\"):\n",
    "        first=get_correct_word(word[:word.index(\"'s\")])\n",
    "        second=\"is\"\n",
    "    elif word.endswith(\"'all\"):\n",
    "        first=get_correct_word(word[:word.index(\"'all\")])\n",
    "        second=\"all\"\n",
    "    else:\n",
    "        first=word\n",
    "        second=\"<allclear>\"\n",
    "    return((first,second))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 616,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanLine(line):\n",
    "    line=line.encode('ascii','ignore').decode(encoding = \"ISO-8859-1\")\n",
    "    line=line.replace('&#36;','DOLLAR').replace('+',' ').replace('$','DOLLAR')\n",
    "    \n",
    "    tokenizer = RegexpTokenizer('\\w+-+\\w+')\n",
    "    hyphenated_word=tokenizer.tokenize(line)\n",
    "    for word in hyphenated_word:\n",
    "        line=line.replace(word,word.replace('-',' '))\n",
    "        \n",
    "    tokenizer = RegexpTokenizer('\\w+,+\\w+')\n",
    "    hyphenated_word=tokenizer.tokenize(line)\n",
    "    for word in hyphenated_word:\n",
    "        line=line.replace(word,word.replace(',',' '))\n",
    "        \n",
    "        \n",
    "    tokenizer = RegexpTokenizer('\\D+/+\\D+')\n",
    "    hyphenated_word=tokenizer.tokenize(line)\n",
    "    for word in hyphenated_word:\n",
    "        line=line.replace(word,word.replace('/',' '))\n",
    "        \n",
    "    table = str.maketrans(dict.fromkeys(string.punctuation))\n",
    "    line=line.translate(table)\n",
    "    \n",
    "#     print(hyphenated_word)\n",
    "\n",
    "    word_lst=line.split()\n",
    "    clean_lst=[]\n",
    "    for idx in range(0,len(word_lst)):\n",
    "        if word_lst[idx] not in punctuation:\n",
    "            dual_words=change_dual_words(word_lst[idx])\n",
    "            if dual_words[1]=='<allclear>':\n",
    "                    clean_lst.append(dual_words[0])\n",
    "            else:\n",
    "                    clean_lst.append(dual_words[0])\n",
    "                    clean_lst.append(dual_words[1])\n",
    "#     processed_lst.append('</s>')\n",
    "    return(str(' ').join(clean_lst))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 617,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_relevance_date(dt):\n",
    "    if dt.weekday()==5:\n",
    "        return(dt - timedelta(1))\n",
    "    elif dt.weekday()==6:\n",
    "        return(dt - timedelta(2))\n",
    "    else:\n",
    "        return(dt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 618,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_datetime = lambda d: datetime.strptime(d, \"%d/%m/%Y\")\n",
    "ft_data=pd.read_csv('../Data/NewsData/financial_times.csv',sep='|',names=['Date', 'HeadLines'],converters={'Date': to_datetime},encoding = \"ISO-8859-1\")\n",
    "mc_data=pd.read_csv('../Data/NewsData/moneycontrol.csv',sep='|',names=['Date', 'HeadLines'],converters={'Date': to_datetime},encoding = \"ISO-8859-1\")\n",
    "op_data=pd.read_csv('../Data/NewsData/oilprice.csv',sep='|',names=['Date', 'HeadLines'],converters={'Date': to_datetime},encoding = \"ISO-8859-1\")\n",
    "mc_data.drop(index=mc_data.iloc[mc_data.HeadLines.isin(['Commodities@moneycontrol \\\\','Commodities@Moneycontrol \\\\','Editor\\'s Take \\\\','Podcast \\\\']).values].index,axis=0,inplace=True)\n",
    "data = pd.concat([ft_data,mc_data,op_data],axis=0,sort=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 619,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5       Oil slips to DOLLAR62 as trade talks between U...\n",
       "83      Oil slips further below DOLLAR58 as economic g...\n",
       "107     Oil falls below DOLLAR64 on prospects for Saud...\n",
       "130     Drone attack on Saudi plants may lift oil pric...\n",
       "148     Oil rises to DOLLAR61 on trade hopes; US inven...\n",
       "                              ...                        \n",
       "1751    Oil slips towards DOLLAR50 on doubts over outp...\n",
       "1763    Oil could collapse into the DOLLAR30s if OPEC ...\n",
       "1793    Oil prices could jump DOLLAR10 a barrel if OPE...\n",
       "6267    Brent crude drops below DOLLAR111 on renewed g...\n",
       "6851    Oil slips to DOLLAR123 after China cuts growth...\n",
       "Name: HeadLines, Length: 85, dtype: object"
      ]
     },
     "execution_count": 619,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mc_data[mc_data.HeadLines.str.contains('\\$')].HeadLines.str.replace('$','DOLLAR')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 620,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Lemmetized_HL']=data.HeadLines.apply(lambda line:cleanLine(line)).str.split().apply(lambda word_lst: ' '+str(' ').join([wnl.lemmatize(word) for word in word_lst])+' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 621,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Agg_Col']=data.Date.apply(lambda dt:datetime.strftime(get_relevance_date(dt),\"%d/%m/%Y\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 622,
   "metadata": {},
   "outputs": [],
   "source": [
    "def upd_word_doc_count(data,full_refresh):\n",
    "    try:\n",
    "        if not full_refresh:\n",
    "            word_lst=data.str.split(expand=True).stack().unique()\n",
    "            word_doc_cnt_df=pd.read_csv('../Data/Processed_Data/word_doc_count.csv',sep='|',index_col=0)\n",
    "            for word in word_lst:\n",
    "                if word in word_doc_cnt_df.index:\n",
    "                    word_doc_cnt_df.DocCount[word]=word_doc_cnt_df.DocCount[word]+data.str.contains(' '+word.replace('$','\\\\$')+' ').sum()\n",
    "                else:\n",
    "                    word_doc_cnt_df.DocCount[word]=data.str.contains(' '+word.replace('$','\\\\$')+' ').sum()\n",
    "            word_doc_cnt_df.to_csv('../Data/Processed_Data/word_doc_count.csv',sep='|')\n",
    "        else:\n",
    "            word_lst=data.str.split(expand=True).stack().unique()\n",
    "            wc={word:data.str.contains(' '+word+' ').sum() for word in word_lst}\n",
    "            pd.DataFrame.from_dict(wc,orient='index',columns=['DocCount']).to_csv('../Data/Processed_Data/word_doc_count.csv',sep='|')\n",
    "    except (pd.errors.EmptyDataError,FileNotFoundError):\n",
    "        exc_type, exc_value, exc_traceback = sys.exc_info()\n",
    "        print('Error found at')\n",
    "        traceback.print_tb(exc_traceback, limit=1, file=sys.stdout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 623,
   "metadata": {},
   "outputs": [],
   "source": [
    "upd_word_doc_count(data.Lemmetized_HL,True)\n",
    "word_doc_cnt_df=pd.read_csv('../Data/Processed_Data/word_doc_count.csv',sep='|',index_col=0)\n",
    "word_doc_cnt_df['IDF']=word_doc_cnt_df.DocCount.apply(lambda dc: math.log(data.shape[0]/dc))\n",
    "word_doc_cnt_df.to_csv('../Data/Processed_Data/word_doc_count.csv',sep='|')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 637,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tf_idf(line,wrdidf):\n",
    "#     word_doc_cnt_df=pd.read_csv('../Data/Processed_Data/word_doc_count.csv',sep='|',index_col=0)\n",
    "    tf_idf_score=0\n",
    "    word_lst=line.split()\n",
    "    for word in word_lst:\n",
    "        tf_idf_score+=(line.count(word)/len(word_lst))*wrdidf[word]\n",
    "    return(tf_idf_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 638,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['TFIDF_SCORE']=data.Lemmetized_HL.apply(lambda line: get_tf_idf(line,word_doc_cnt_df['IDF']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 639,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_score=data.groupby(['Agg_Col']).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 641,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_score.to_csv('../Data/Processed_Data/tfidf_score.csv',sep='|')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
