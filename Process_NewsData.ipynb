{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from string import punctuation\n",
    "import random\n",
    "import math\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import movie_reviews\n",
    "from nltk import word_tokenize\n",
    "from textblob import TextBlob\n",
    "from nltk.util import ngrams\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem import SnowballStemmer\n",
    "from spellchecker import SpellChecker\n",
    "from datetime import datetime, timedelta\n",
    "import re,string,math\n",
    "import gensim\n",
    "from gensim import corpora, models\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "wnl = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def change_dual_words(word):\n",
    "    first=''\n",
    "    second=''\n",
    "    def get_correct_word(w):\n",
    "        if w==\"sha\":\n",
    "            return(\"shall\")\n",
    "        elif w==\"wo\":\n",
    "            return(\"will\")\n",
    "        elif w==\"y\":\n",
    "            return(\"you\")\n",
    "        else:\n",
    "            return(w)\n",
    "#     word=word.replace('$','$').replace('+','')\n",
    "    word=word.strip().strip(':').strip(\"'\").strip(',').strip('`').strip('[').strip(']').strip('-').strip('_').strip('#').lower()\n",
    "    word=word.strip('?').strip('+').strip('/').strip('(').strip(')').strip(';').strip('!').strip('.').strip('?')\n",
    "    if word.endswith(\"n't\"):\n",
    "        first=get_correct_word(word[:word.index(\"n't\")])\n",
    "        second=\"not\"\n",
    "    elif word.endswith(\"'ll\"):\n",
    "        first=get_correct_word(word[:word.index(\"'ll\")])\n",
    "        second=\"will\"\n",
    "    elif word.endswith(\"'ve\"):\n",
    "        first=get_correct_word(word[:word.index(\"'ve\")])\n",
    "        second=\"have\"\n",
    "    elif word.endswith(\"'re\"):\n",
    "        first=get_correct_word(word[:word.index(\"'re\")])\n",
    "        second=\"are\"\n",
    "    elif word.endswith(\"'d\"):\n",
    "        first=get_correct_word(word[:word.index(\"'d\")])\n",
    "        second=\"did\"\n",
    "    elif word.endswith(\"'s\"):\n",
    "        first=get_correct_word(word[:word.index(\"'s\")])\n",
    "        second=\"is\"\n",
    "    elif word.endswith(\"'all\"):\n",
    "        first=get_correct_word(word[:word.index(\"'all\")])\n",
    "        second=\"all\"\n",
    "    else:\n",
    "        first=word\n",
    "        second=\"<allclear>\"\n",
    "    return((first,second))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def cleanLine(line):\n",
    "    line=line.encode('ascii','ignore').decode(encoding = \"ISO-8859-1\")\n",
    "    line=line.replace('&#36;','DOLLAR').replace('+',' ').replace('$','DOLLAR')\n",
    "    remove_digits=str.maketrans('', '', string.digits)\n",
    "    line=line.translate(remove_digits)\n",
    "    \n",
    "    tokenizer = RegexpTokenizer('\\w+-+\\w+')\n",
    "    hyphenated_word=tokenizer.tokenize(line)\n",
    "    for word in hyphenated_word:\n",
    "        line=line.replace(word,word.replace('-',' '))\n",
    "        \n",
    "    tokenizer = RegexpTokenizer('\\w+,+\\w+')\n",
    "    hyphenated_word=tokenizer.tokenize(line)\n",
    "    for word in hyphenated_word:\n",
    "        line=line.replace(word,word.replace(',',' '))\n",
    "        \n",
    "        \n",
    "    tokenizer = RegexpTokenizer('\\D+/+\\D+')\n",
    "    hyphenated_word=tokenizer.tokenize(line)\n",
    "    for word in hyphenated_word:\n",
    "        line=line.replace(word,word.replace('/',' '))\n",
    "        \n",
    "    table = str.maketrans(dict.fromkeys(string.punctuation))\n",
    "    line=line.translate(table)\n",
    "    \n",
    "#     print(hyphenated_word)\n",
    "    stopword = stopwords.words('english')\n",
    "    stopword.append(['aa','aah','aahhs','aahs','(',')',\"''\",',','``','-','.','4','2','3po', '6x9', '8mm','5','9','26','$$','$@^@','&#','&nbsp','&quot','#@%$^','==','>from','>honk','@#$^%','@#%','@$&%','a&m','a&w','a+'])\n",
    "#     stopwordList=set(stopword)\n",
    "    \n",
    "    word_lst=line.split()\n",
    "    clean_lst=[]\n",
    "    for idx in range(0,len(word_lst)):\n",
    "        if word_lst[idx] not in punctuation and word_lst[idx] not in stopword:\n",
    "            dual_words=change_dual_words(word_lst[idx])\n",
    "            if dual_words[1]=='<allclear>':\n",
    "                    clean_lst.append(dual_words[0])\n",
    "            else:\n",
    "                if dual_words[0] not in stopword:\n",
    "                    clean_lst.append(dual_words[0])\n",
    "                if dual_words[1] not in stopword:\n",
    "                    clean_lst.append(dual_words[1])\n",
    "#     processed_lst.append('</s>')\n",
    "    line=str(' ').join(clean_lst)\n",
    "#     line=str(TextBlob(line).correct())\n",
    "    return(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_relevance_date(dt):\n",
    "    if dt.weekday()==5:\n",
    "        return(dt - timedelta(1))\n",
    "    elif dt.weekday()==6:\n",
    "        return(dt - timedelta(2))\n",
    "    else:\n",
    "        return(dt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Check_Dates(df,header):\n",
    "    print('Counts for '+header)\n",
    "    print('Required no. of Days: ',(df.Date.max()-df.Date.min()).days,\\\n",
    "          'Existing no. of Days: ',len(df.Date.unique()),\\\n",
    "          'Missing no. Of Days: ',(df.Date.max()-df.Date.min()).days-len(df.Date.unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counts for Financial Times\n",
      "Required no. of Days:  1467 Existing no. of Days:  1285 Missing no. Of Days:  182\n",
      "Counts for Money Control\n",
      "Required no. of Days:  3323 Existing no. of Days:  2643 Missing no. Of Days:  680\n",
      "Counts for Oilprice.com\n",
      "Required no. of Days:  3841 Existing no. of Days:  1768 Missing no. Of Days:  2073\n",
      "Counts for Investing.com\n",
      "Required no. of Days:  3365 Existing no. of Days:  1762 Missing no. Of Days:  1603\n",
      "Counts for Combined\n",
      "Required no. of Days:  3851 Existing no. of Days:  3074 Missing no. Of Days:  777\n"
     ]
    }
   ],
   "source": [
    "to_datetime = lambda d: datetime.strptime(d, \"%d/%m/%Y\")\n",
    "ft_data=pd.read_csv('../Data/NewsData/financial_times.csv',sep='|',names=['Date', 'HeadLines'],converters={'Date': to_datetime},encoding = \"ISO-8859-1\")\n",
    "mc_data=pd.read_csv('../Data/NewsData/moneycontrol.csv',sep='|',names=['Date', 'HeadLines'],converters={'Date': to_datetime},encoding = \"ISO-8859-1\")\n",
    "op_data=pd.read_csv('../Data/NewsData/oilprice.csv',sep='|',names=['Date', 'HeadLines'],converters={'Date': to_datetime},encoding = \"ISO-8859-1\")\n",
    "inv_data=pd.read_csv('../Data/NewsData/investing.csv',sep='|',names=['Date', 'HeadLines'],converters={'Date': to_datetime},encoding = \"ISO-8859-1\")\n",
    "mc_data.drop(index=mc_data.iloc[mc_data.HeadLines.isin(['Commodities@moneycontrol \\\\','Commodities@Moneycontrol \\\\','Editor\\'s Take \\\\','Podcast \\\\']).values].index,axis=0,inplace=True)\n",
    "data = pd.concat([ft_data,mc_data,op_data,inv_data],axis=0,sort=False,ignore_index=True)\n",
    "data=data.drop_duplicates()\n",
    "Check_Dates(ft_data,'Financial Times')\n",
    "Check_Dates(mc_data,'Money Control')\n",
    "Check_Dates(op_data,'Oilprice.com')\n",
    "Check_Dates(inv_data,'Investing.com')\n",
    "Check_Dates(data,'Combined')\n",
    "# print((op_data.Date.max()-op_data.Date.min()).days,len(op_data.Date.unique()),(op_data.Date.max()-op_data.Date.min()).days-len(op_data.Date.unique()))\n",
    "# print((mc_data.Date.max()-mc_data.Date.min()).days,len(mc_data.Date.unique()),(mc_data.Date.max()-mc_data.Date.min()).days-len(mc_data.Date.unique()))\n",
    "# print((ft_data.Date.max()-ft_data.Date.min()).days,len(ft_data.Date.unique()),(ft_data.Date.max()-ft_data.Date.min()).days-len(ft_data.Date.unique()))\n",
    "# print((inv_data.Date.max()-inv_data.Date.min()).days,len(inv_data.Date.unique()),(inv_data.Date.max()-inv_data.Date.min()).days-len(inv_data.Date.unique()))\n",
    "# print((data.Date.max()-data.Date.min()).days,len(data.Date.unique()),(data.Date.max()-data.Date.min()).days-len(data.Date.unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "to_datetime = lambda d: datetime.strptime(d, \"%Y-%m-%d\")\n",
    "calc_movement= lambda mv: 'down' if mv<0 else ('up' if mv>0 else 'nomovement')\n",
    "data['Agg_Col']=data.Date.apply(lambda dt:get_relevance_date(dt))\n",
    "price_data=pd.read_csv('../Data/NewsData/CrudePriceSmoothened.csv',sep='|',converters={'Date': to_datetime},encoding = \"ISO-8859-1\")\n",
    "# price_data['NEXT_DAY']=price_data.sort_values('DATE',ascending=False,kind='mergesort')['CLOSING_PRICE'].shift(1)\n",
    "# price_data['DIFF']=price_data.sort_values('DATE',ascending=False,kind='mergesort')['CLOSING_PRICE'].shift(1)-price_data['CLOSING_PRICE']\n",
    "price_data['MoveMent']=(price_data.sort_values('Date',ascending=False,kind='mergesort')['CP_trend'].shift(1)-price_data['CP_trend']).apply(calc_movement)\n",
    "data=data.join(price_data[['Date','MoveMent','CP_trend']].set_index('Date'), on='Agg_Col',how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data['Cleaned_HL']=data.HeadLines.apply(lambda line:cleanLine(line))\n",
    "data['Lemmetized_HL']=data.Cleaned_HL.str.split().apply(lambda word_lst: str(' ').join([wnl.lemmatize(word) for word in word_lst]))\n",
    "processed_df=data.Cleaned_HL.apply(lambda line: TextBlob(line).sentiment).apply(pd.Series)\n",
    "processed_df.columns=['Polarity','Subjectivity']\n",
    "data=pd.concat([data,processed_df],axis=1,sort=False)\n",
    "# processed_df['Date']=data.Date\n",
    "# from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# vectorizer=TfidfVectorizer(use_idf=True,stop_words=['aa','aaa'])\n",
    "# fitted_vec=vectorizer.fit_transform(data.Lemmetized_HL)\n",
    "# tf_idf_df=pd.DataFrame(fitted_vec.todense(),columns=vectorizer.get_feature_names())\n",
    "# processed_df = pd.concat([processed_df,tf_idf_df],axis=1,sort=False)\n",
    "data[['Date','Cleaned_HL','MoveMent']].to_csv('../Data/Processed_Data/input_data_for_cnn.csv',sep='|',index=False)\n",
    "data[['Date','Agg_Col','Lemmetized_HL','MoveMent','Polarity','Subjectivity']].to_csv('../Data/Processed_Data/input_data_for_lda.csv',sep='|',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'oil|us|opec|saudi|crude|energy|shale|gas|dollar|iran|could|stocks|dollarbn|premium|russia|week|arabia|bp|supply|china|sea|aramco|rise|peak|shell|trump|pipeline|uk|sanctions|brent|venezuela|investors|growth|producers|iea|nigeria|drilling|gulf|rosneft|russian|saudis|barrel|iraq|fracking|bpd|glut|fuel|mexico|eia|chevron|chinas|libya|opecs|iranian|permian|canadian|chinese|reserves|arctic|petrobras|gasoline|russias|texas|oils|tullow|lng|surge|venezuelas|canadas|canada|barrels|oilfield|mexicos|irans|asian|natural|carbon|arabias|norway|drillers|european|fossil|libyan|gazprom|fuels|iraqi|refiners|norways|petroleum|tankers|nigerias|libyas|wti|venezuelan|emissions|repsol|statoil|brazil|mexican|electric|refinery|kurdistan|nigerian|australian|refining|petrol|shows|iraqs|kuwait|syria|kurdish|kazakhstan|coal|turkey|refineries|uae|petrofac|lukoil'"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "jd=pd.concat([ft_data,op_data],axis=0,sort=False,ignore_index=True)\n",
    "jd['Cleaned_HL']=jd.HeadLines.apply(lambda line:cleanLine(line))\n",
    "stopword = stopwords.words('english')\n",
    "stopword.extend(['says','fall','back','hit','high','since','low','set','profits','first','higher','despite','rally','drop',\n",
    "                 'record','boost','growth''could','new','cut','demand','boom','exports','industry','cuts','count','global',\n",
    "                 'may','year','major','north','prices','production','big','market','markets','price','world','worlds',\n",
    "                 'output','deal','next','inventories','time','majors','ahead','rig','lower','hits','groups','biggest','ipo',\n",
    "                 'group','asia','falls','sector','offshore','amid','shares','years','chief','future','warns','trade','stake',\n",
    "                 'investment','companies','sell','recovery','draw','profit','war','month','gains','rises','total','traders',\n",
    "                 'jump','buy','end','slide','rebound','plans','decline','inventory','climate','risk','build','trading',\n",
    "                 'minister','assets','exxon','fund','still','meeting','th','billion','forecast','report','business','data',\n",
    "                 'east','rising','day','plan','looks','debt','company','sands','field','reports','export','change','crisis',\n",
    "                 'fears','earnings','takes','review','loss','faces','tax','premier','another','dollarm','funds','raises',\n",
    "                 'million','eyes','imports','top','sale','sees','threat','drops','last','talks','boosts','pressure','seeks',\n",
    "                 'face','exxonmobil','spending','largest','look','pemex','bond','two','one','opening','bullish','much',\n",
    "                 'take','keep','tanker','forecasts','live','project','keystone','vitol','extends','pay','middle','commodity',\n",
    "                 'hedge','highest','power','continues','push','commodities','quote','long','q','hopes','india','south',\n",
    "                 'eni','expected','key','threatens','green','slump','latest','strong','worst','best','soon','needs','go',\n",
    "                 'well','potential','five','outlook','state','play','losses','near','trader','increase','bid','wall','move',\n",
    "                 'bets','economic','raise','banks','second','wood','makes','sp','rules','share','trumps','quarter',\n",
    "                 'trafigura','producer','wont','lead','slips','europe','minute','need','three','exploration','become',\n",
    "                 'falling','start','bhp','april','see','goldman','make','close','attack','takeover','ban','non','sells',\n",
    "                 'march','economy','crash','attacks','discovery','continue','bakken','way','slides','break','poised',\n",
    "                 'services','third','concerns','watch','target','battle','shells','focus','case','cost','ceo','level','deals',\n",
    "                 'huge','post','freeze','bet','jumps','calls','find','must','climbs','equities','coming','plunge','get',\n",
    "                 'costs','return','fails','good','help','worries','targets','gain','genel','dividend','head','weekly','behind',\n",
    "                 'shift','stream','extend','really','january','hurricane','months','point','signs','strategy','pushes',\n",
    "                 'offer','ready','agree','glencore','far','tough','corruption','part','centrica','deepwater','winners','cuadrilla',\n",
    "                 'slip','capacity','policy','cheap','foreign','strike','turn','sends','unit','eye','coast','fire','storm',\n",
    "                 'game','cash','lifts','tensions','step','anadarko','shipping','agrees','cant','cap','projects','wins',\n",
    "                 'fed','bn','holds','harvey','america','short','support','deeper','lowest','bonds','junk','schlumberger',\n",
    "                 'aims','weak','surprise','november','straight','double','rout','term','nord','xl','expect','slowdown',\n",
    "                 'puts','halliburton','eu','swings','buys','hold','strikes','dip','climb','drive','investor','might',\n",
    "                 'national','capital','election','money','fresh','weeks','gold','giant','questions','away','loses','debate',\n",
    "                 'discoveries','extension','turns','taking','struggle','signals','rates','wealth','st','decision','launches',\n",
    "                 'risks','impact','returns','fight','brexit','bearish','half','volatility','dakota','beyond','independence',\n",
    "                 'enquest','supplies','american','futures','track','july','rallies','forces','opportunity','occidental',\n",
    "                 'meet','government','towards','sales','lose','moves','real','going','rush','stock','small','numbers',\n",
    "                 'street','geopolitical','cairn','bust','bank','africa','amec','per','less','stockpiles','fields','nuclear',\n",
    "                 'sets','life','conocophillips','pipelines','chairman','aramcos','cutting','enough','june','pump','come',\n",
    "                 'british','strategic','slow','work','political','rigs','almost','decade','add','brazils','rebounds','west'\n",
    "                 'shows','whats','fast','february','obama','abu','norwegian','dont','struggles','spot','balance','dispute',\n",
    "                 'finds','tops','collapse','court','auction','bear','imf','probe','beat','net','analysts','action','open',\n",
    "                 'never','putin','compliance','angola','dutch','streak','value','president','highs','claims','longer','large',\n",
    "                 'hard','weaker','significant','great','call','keeps','lift','surges','london','land','ends','looking','grow',\n",
    "                 'breaks','massive','losers','storage','right','trillion','revolution','santos','becomes','posts','boss',\n",
    "                 'listing','boosted','alberta','september','even','challenge','recover','alliance','isnt','blow','tech',\n",
    "                 'steady','remain','yet','stop','jobs','test','black','west','gives','volatile','rights','talk','making',\n",
    "                 'sign','slumps','force','problems','transition','royal','miss','revenues','expectations','moodys','bounce',\n",
    "                 'finally','offers','currencies','corporate','curb','asset','retreat','bigger','dilemma','opens','cartel',\n",
    "                 'reach','august','crunch','soars','better','sharply','bulls','shake','urges','stay','doha','backs','private',\n",
    "                 'buying','equity','optimism','credit','drill','leads','ineos','kirkuk','losing','halt','ft','left','ease',\n",
    "                 'without','weighs','yen','former','hope','give','gets','maersk','launch','woes','results','put','pre',\n",
    "                 'resources','spike','points','role','tuesday','ups','cars','run','begin','international','would','financial',\n",
    "                 'four','heavy','clean','save','contract','japan','era','security','mark','issue','vows','reliance','invest',\n",
    "                 'bps','reforms','survive','york','hotspot','budget','vote','levels','legal','win','nears','board','announces',\n",
    "                 'things','remains','patch','likely','reality','ghana','free','dollartn','halts','comeback','warning','venture',\n",
    "                 'goes','comes','frontier','problem','baker','gunvor','doubles','flow','december','pain','hughes','firstft'])\n",
    "stopwordList=set(tuple(stopword))\n",
    "word_list=jd.Cleaned_HL.str.split().explode().reset_index().set_index('Cleaned_HL').drop(stopwordList,errors=None).reset_index().drop('index',axis=1)\n",
    "word_list=word_list.groupby('Cleaned_HL')['Cleaned_HL'].count()\\\n",
    "        .rename(columns={'Cleaned_HL':'Word'})\\\n",
    "        .reset_index()\n",
    "word_list.columns=['word','cnt']\n",
    "word_list.sort_values('cnt',ascending=False,inplace=True)\n",
    "'|'.join(word_list[word_list.cnt>=15].head(150).word.to_list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>cnt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3906</th>\n",
       "      <td>oil</td>\n",
       "      <td>5176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6268</th>\n",
       "      <td>us</td>\n",
       "      <td>1177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3937</th>\n",
       "      <td>opec</td>\n",
       "      <td>696</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5015</th>\n",
       "      <td>saudi</td>\n",
       "      <td>564</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1315</th>\n",
       "      <td>crude</td>\n",
       "      <td>541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6091</th>\n",
       "      <td>turkey</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1062</th>\n",
       "      <td>coal</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6116</th>\n",
       "      <td>uae</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3450</th>\n",
       "      <td>lukoil</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4162</th>\n",
       "      <td>petrofac</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>118 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          word   cnt\n",
       "3906       oil  5176\n",
       "6268        us  1177\n",
       "3937      opec   696\n",
       "5015     saudi   564\n",
       "1315     crude   541\n",
       "...        ...   ...\n",
       "6091    turkey    17\n",
       "1062      coal    17\n",
       "6116       uae    16\n",
       "3450    lukoil    15\n",
       "4162  petrofac    15\n",
       "\n",
       "[118 rows x 2 columns]"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# [x for x in word_list[word_list.cnt<=17].head(150).word]\n",
    "\n",
    "# word_list[word_list.cnt<=17].head(50)\n",
    "word_list[word_list.cnt>=15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15858, 10)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
